{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258339e5",
   "metadata": {},
   "source": [
    "# RAG SYSTEM LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4826a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f284f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find this zipfile.\n"
     ]
    }
   ],
   "source": [
    "# Unzip the data files\n",
    "import zipfile\n",
    "def unzip_file(path, data_dir, delete=True):\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Cannot find this zipfile.\")\n",
    "        return\n",
    "    \n",
    "    if path.endswith(\".zip\"):\n",
    "        with zipfile.ZipFile(path, \"r\") as zipref:\n",
    "            zipref.extractall(data_dir)\n",
    "            print(\"Unzip succesfully to:\",data_dir)\n",
    "        if delete:\n",
    "            os.remove(path)\n",
    "            print(\"Deleted zipfile.\")\n",
    "            return\n",
    "    else:\n",
    "        print(f\"This format file is not accepted: {path}\")\n",
    "\n",
    "data_dir = \"../data\"\n",
    "path = f\"{data_dir}/knowledge-base.zip\"\n",
    "\n",
    "unzip_file(path=path,data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a605b5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/knowledge-base\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data\"\n",
    "data_path = os.path.join(data_dir, \"knowledge-base\")\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a34ac5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", 'your_api_key_here')\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "MODEL_EMBEDDING = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "TEMPERATURE = 0.1\n",
    "# Add this check!\n",
    "if not OPENAI_API_KEY:\n",
    "    # If the key is not found, stop the program with a clear error\n",
    "    raise ValueError(\"ðŸ”´ ERROR: The OPENAI_API_KEY environment variable is not set!\")\n",
    "\n",
    "# If the key is found, assign it and print a success message\n",
    "OpenAI.api_key = OPENAI_API_KEY\n",
    "print(\"OpenAI API Key loaded successfully.\")\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca5450",
   "metadata": {},
   "source": [
    "## Add data to the context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5abdf",
   "metadata": {},
   "source": [
    "### Add employees data to the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b34500",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {}\n",
    "employees_dir = os.path.join(data_path, \"employees/*\")\n",
    "employees = glob.glob(employees_dir)\n",
    "\n",
    "for employee in employees:\n",
    "    name = employee.split(' ')[-1][:-3]\n",
    "    doc = \"\"\n",
    "    with open(employee, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "    context[name]=doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb5f82e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Carter', 'Greene', 'Thomson', 'Tran', 'Thompson', 'Chen', 'Trenton', 'Harper', 'Blake', 'Lancaster', 'Bishop', 'Spencer'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f925460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Avery Lancaster\\n\\n## Summary\\n- **Date of Birth**: March 15, 1985  \\n- **Job Title**: Co-Founder & Chief Executive Officer (CEO)  \\n- **Location**: San Francisco, California  \\n\\n## Insurellm Career Progression\\n- **2015 - Present**: Co-Founder & CEO  \\n  Avery Lancaster co-founded Insurellm in 2015 and has since guided the company to its current position as a leading Insurance Tech provider. Avery is known for her innovative leadership strategies and risk management expertise that have catapulted the company into the mainstream insurance market.  \\n\\n- **2013 - 2015**: Senior Product Manager at Innovate Insurance Solutions  \\n  Before launching Insurellm, Avery was a leading Senior Product Manager at Innovate Insurance Solutions, where she developed groundbreaking insurance products aimed at the tech sector.  \\n\\n- **2010 - 2013**: Business Analyst at Edge Analytics  \\n  Prior to joining Innovate, Avery worked as a Business Analyst, focusing on market trends and consumer preferences in the insurance space. This position laid the groundwork for Averyâ€™s future entrepreneurial endeavors.\\n\\n## Annual Performance History\\n- **2015**: **Exceeds Expectations**  \\n  Averyâ€™s leadership during Insurellm's foundational year led to successful product launches and securing initial funding.  \\n\\n- **2016**: **Meets Expectations**  \\n  Growth continued, though challenges arose in operational efficiency that required Avery's attention.  \\n\\n- **2017**: **Developing**  \\n  Market competition intensified, and monthly sales metrics were below targets. Avery implemented new strategies which required a steep learning curve.  \\n\\n- **2018**: **Exceeds Expectations**  \\n  Under Averyâ€™s pivoted vision, Insurellm launched two new successful products that significantly increased market share.  \\n\\n- **2019**: **Meets Expectations**  \\n  Steady growth, however, some team tensions led to a minor drop in employee morale. Avery recognized the need to enhance company culture.  \\n\\n- **2020**: **Below Expectations**  \\n  The COVID-19 pandemic posed unforeseen operational difficulties. Avery faced criticism for delayed strategy shifts, although efforts were eventually made to stabilize the company.  \\n\\n- **2021**: **Exceptional**  \\n  Avery's decisive transition to remote work and rapid adoption of digital tools led to record-high customer satisfaction levels and increased sales.  \\n\\n- **2022**: **Satisfactory**  \\n  Avery focused on rebuilding team dynamics and addressing employee concerns, leading to overall improvement despite a saturated market.  \\n\\n- **2023**: **Exceeds Expectations**  \\n  Market leadership was regained with innovative approaches to personalized insurance solutions. Avery is now recognized in industry publications as a leading voice in Insurance Tech innovation.\\n\\n## Compensation History\\n- **2015**: $150,000 base salary + Significant equity stake  \\n- **2016**: $160,000 base salary + Equity increase  \\n- **2017**: $150,000 base salary + Decrease in bonus due to performance  \\n- **2018**: $180,000 base salary + performance bonus of $30,000  \\n- **2019**: $185,000 base salary + market adjustment + $5,000 bonus  \\n- **2020**: $170,000 base salary (temporary reduction due to COVID-19)  \\n- **2021**: $200,000 base salary + performance bonus of $50,000  \\n- **2022**: $210,000 base salary + retention bonus  \\n- **2023**: $225,000 base salary + $75,000 performance bonus  \\n\\n## Other HR Notes\\n- **Professional Development**: Avery has actively participated in leadership training programs and industry conferences, representing Insurellm and fostering partnerships.  \\n- **Diversity & Inclusion Initiatives**: Avery has championed a commitment to diversity in hiring practices, seeing visible improvements in team representation since 2021.  \\n- **Work-Life Balance**: Feedback revealed concerns regarding work-life balance, which Avery has approached by implementing flexible working conditions and ensuring regular check-ins with the team.\\n- **Community Engagement**: Avery led community outreach efforts, focusing on financial literacy programs, particularly aimed at underserved populations, improving Insurellm's corporate social responsibility image.  \\n\\nAvery Lancaster has demonstrated resilience and adaptability throughout her career at Insurellm, positioning the company as a key player in the insurance technology landscape.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context[\"Lancaster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73e61cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedContextRetriever:\n",
    "    def __init__(self, paths_list, model_name=MODEL_GPT):\n",
    "        print(f\"Loading embedding model '{model_name}'...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.documents = self._load_index_documents(paths_list)\n",
    "\n",
    "    def _load_index_documents(self, paths_list):\n",
    "        all_docs = []\n",
    "        for path_parttern in paths_list:\n",
    "            source_type = os.path.basename(os.path.dirname(path_parttern))\n",
    "            \n",
    "            for file_path in glob.glob(path_parttern):\n",
    "                name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    all_docs.append({\n",
    "                        \"name\": name,\n",
    "                        \"content\": content,\n",
    "                        \"source\": source_type\n",
    "                    })\n",
    "\n",
    "        # create embedding for all of documents\n",
    "        print(f\"Creating embeddings for {len(all_docs)} documents...\")\n",
    "        contents = [doc['content'] for doc in all_docs]\n",
    "        embeddings = self.model.encode(contents, show_progress_bar=True)\n",
    "        \n",
    "        # Gáº¯n embedding vÃ o láº¡i má»—i document\n",
    "        for i, doc in enumerate(all_docs):\n",
    "            doc['embedding'] = embeddings[i]\n",
    "            \n",
    "        return all_docs\n",
    "        \n",
    "    # def load_documents(self):\n",
    "    #     documents = {}\n",
    "    #     for file_path in glob.glob(self.docs_path):\n",
    "    #         name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    #         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    #             documents[name] = file.read()\n",
    "    #     return documents\n",
    "\n",
    "    def retrieve(self, query, top_k=5):\n",
    "        if self.documents == None:\n",
    "            return []\n",
    "        \n",
    "        # Create a embedding for each query\n",
    "        query_embedding = self.model.encode(query)\n",
    "        \n",
    "        # Get all of document embeddings\n",
    "        doc_embeddings = np.array([doc['embedding'] for doc in self.documents])\n",
    "        \n",
    "        # Score with cosine_scores\n",
    "        cosine_scores = np.dot(doc_embeddings, query_embedding) / (np.linalg.norm(doc_embeddings, axis=1) * np.linalg.norm(query_embedding))\n",
    "        top_k_indices = np.argsort(cosine_scores)[-top_k:][::-1]\n",
    "        \n",
    "        # Return results of query\n",
    "        res = []\n",
    "        for i in top_k_indices:\n",
    "            doc = self.documents[i]\n",
    "            res.append({\n",
    "                \"name\": doc['name'],\n",
    "                \"content\": doc['content'],\n",
    "                \"source\": doc['source'],\n",
    "                \"score\": cosine_scores[i]\n",
    "            })\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def add_context(self, title, details):\n",
    "        self.context[title] = details\n",
    "\n",
    "    def get_relevant_context(self, message):\n",
    "        relevant_context = []\n",
    "        msg_lower = message.lower()\n",
    "        for context_title, context_details in self.context.items():\n",
    "            title_processed = context_title.lower().replace(\"_\", \" \")\n",
    "            if any(word in msg_lower for word in title_processed.split()):\n",
    "                relevant_context.append(context_details)\n",
    "        return relevant_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85340bbb",
   "metadata": {},
   "source": [
    "### Define all data as path and load it to paths list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81b4d185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 of '../data/knowledge-base/employees/*.md'\n",
      "4 of '../data/knowledge-base/products/*.md'\n",
      "12 of '../data/knowledge-base/contracts/*.md'\n",
      "3 of '../data/knowledge-base/company/*.md'\n"
     ]
    }
   ],
   "source": [
    "employees_dir = os.path.join(data_path, \"employees\", \"*.md\")\n",
    "products_dir = os.path.join(data_path, \"products\", \"*.md\")\n",
    "contracts_dir = os.path.join(data_path, \"contracts\", \"*.md\")\n",
    "company_dir = os.path.join(data_path, \"company\", \"*.md\")\n",
    "\n",
    "all_data_paths = [employees_dir, products_dir, contracts_dir, company_dir]\n",
    "\n",
    "for path_pattern in all_data_paths:\n",
    "    matching_files = glob.glob(path_pattern)\n",
    "    if matching_files:\n",
    "        print(f\"{len(matching_files)} of '{path_pattern}'\")\n",
    "    else:\n",
    "        print(f\"Do not find any markdown file of '{path_pattern}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2adf3e",
   "metadata": {},
   "source": [
    "## Initialize the embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0374d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'...\n",
      "Creating embeddings for 31 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b9193bad2b432c8cf96426594d1808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "retrieve = UnifiedContextRetriever(paths_list=all_data_paths,model_name=MODEL_EMBEDDING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625b9b4",
   "metadata": {},
   "source": [
    "## Going to query the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c26b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in source 'employees' (File: Avery Lancaster) with score 0.3093:\n",
      "# Avery Lancaster\n",
      "\n",
      "## Summary\n",
      "- **Date of Birth**: March 15, 1985  \n",
      "- **Job Title**: Co-Founder & Chief Executive Officer (CEO)  \n",
      "- **Location**: San Francisco, California  \n",
      "\n",
      "## Insurellm Career Progr...\n",
      "\n",
      "Found in source 'company' (File: about) with score 0.1740:\n",
      "# About Insurellm\n",
      "\n",
      "Insurellm was founded by Avery Lancaster in 2015 as an insurance tech startup designed to disrupt an industry in need of innovative products. It's first product was Markellm, the ma...\n",
      "\n",
      "Found in source 'employees' (File: Samantha Greene) with score 0.1390:\n",
      "# Samantha Greene\n",
      "\n",
      "## Summary\n",
      "- **Date of Birth:** October 14, 1990\n",
      "- **Job Title:** HR Generalist\n",
      "- **Location:** Denver, Colorado\n",
      "\n",
      "## Insurellm Career Progression\n",
      "- **2020** - Joined Insurellm as a ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "msg = \"Tell me everything about Lancaster\"\n",
    "results = retrieve.retrieve(msg, top_k=3)\n",
    "for res in results:\n",
    "    print(f\"Found in source '{res['source']}' (File: {res['name']}) with score {res['score']:.4f}:\\n{res['content'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77844b4b",
   "metadata": {},
   "source": [
    "## Transfer the knowledge to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e048f30",
   "metadata": {},
   "source": [
    "### Load product data to the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43e3541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an expert in answering accurate questions about Insurellm, the Insurance Tech company. Give brief, accurate answers. If you don't know the answer, say so. Do not make anything up if you haven't been provided with relevant context.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc7583ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    # retrieve from store\n",
    "    re_res = retrieve.retrieve(query=message, top_k=3)\n",
    "\n",
    "    # Augmenting found information into prompt\n",
    "    formatted_context = \"\"\n",
    "    if re_res:\n",
    "        formatted_context += \"Base on this information:\\n---\\n\"\n",
    "        for result in re_res:\n",
    "            formatted_context += f\"[Source: {result['source']}/{result['name']}]\\n{result['content']}\\n---\\n\"\n",
    "        formatted_context += \"Let's answer the questions of user.\"\n",
    "\n",
    "    augmented_message = f\"{formatted_context}\\n\\nQuestion: {message}\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    for human, ai in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": human})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ai})\n",
    "    messages.append({\"role\": \"user\", \"content\": augmented_message})\n",
    "    \n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            response += chunk.choices[0].delta.content\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e287d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Gradio Interface...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27450/3388319053.py:11: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "/home/dikhangcshcmut/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py:331: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Launching Gradio Interface...\")\n",
    "view = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    title=\"ðŸ¤– Smart RAG Chatbot\",\n",
    "    description=\"Ask me anything about the company's internal data.\",\n",
    "    examples=[\n",
    "        \"Tell me about Avery Lancaster\",\n",
    "        \"What are the main features of the Lancaster Sofa?\",\n",
    "        \"Summarize the company's remote work policy\"\n",
    "    ],\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    ").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
